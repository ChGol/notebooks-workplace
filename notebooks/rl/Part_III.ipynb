{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[SOLVED] Crash Course in Reinforcement Learning - Part III","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Gw3P1oFTOlnF","colab_type":"text"},"source":["#  Setup\n","Run below cells and hide it afterwards with the arrow on the left. "]},{"cell_type":"code","metadata":{"id":"S-4YPGE9O9mH","colab_type":"code","colab":{}},"source":["!pip install gym[Box2D] pyvirtualdisplay pyglet > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xixLPph_JNnw","colab_type":"code","outputId":"65c0d8b6-caac-44fd-db99-ec08361b849f","executionInfo":{"status":"ok","timestamp":1574427889601,"user_tz":-60,"elapsed":24269,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import gym\n","from gym import logger as gymlogger\n","from gym.wrappers import Monitor\n","gymlogger.set_level(40) #error only\n","import numpy as np\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)\n","import math\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","\n","from typing import List, Tuple\n","\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from collections import deque\n","\n","from IPython import display as ipythondisplay\n","from IPython.display import display, update_display, clear_output\n","from time import sleep\n","\n","from pyvirtualdisplay import Display\n","xdisplay = Display(visible=0, size=(1300, 900), backend=\"xvfb\")\n","xdisplay.start()\n","\n","\n","\"\"\"\n","Utility functions to enable video recording of gym environment and displaying it\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","class DoneWrapper(gym.Wrapper):\n","\n","  def step(self, action):\n","    observation, reward, done, info = self.env.step(action) \n","    return observation, reward, False, info\n","      \n","\n","def show_video():\n","  mp4list = glob.glob('video/*.mp4')\n","  if len(mp4list) > 0:\n","    mp4 = mp4list[0]\n","    video = io.open(mp4, 'r+b').read()\n","    encoded = base64.b64encode(video)\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","  else: \n","    print(\"Could not find video\")\n","    \n","    \n","def wrap_env(env, done=True):\n","  if not done:\n","    env = DoneWrapper(env)\n","  env = Monitor(env, './video', force=True, mode='evaluation')\n","  return env\n","\n","\n","def print_ansi(screen, display_id='42', wait=0.5):\n","    clear_output(wait=True)\n","    update_display(print(screen.getvalue()), display_id=display_id)\n","    sleep(wait)\n","\n","\n","def plot(img):\n","  fig = plt.figure(figsize=(8,6))\n","  ax = fig.add_subplot(111)\n","  ax.imshow(img)\n","  ax.set_xticks([])\n","  ax.set_yticks([])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"w400JFAILnGQ","colab_type":"code","colab":{}},"source":["def gather_trajectories(env: gym.Env, policy, num_trajs: int = 10):\n","    \"\"\"Gather `num_trajs` trajectories by interacting with the environment using the given policy.\"\"\"\n","    \n","    # preapre a list for the trajectories\n","    history = []\n","    \n","    for traj_idx in range(num_trajs):\n","        obs = env.reset()\n","        done = False\n","        current_traj = []\n","        while not done:\n","            \n","            # sample an action from the policy\n","            action = policy.sample(obs)\n","            # feed it into the environment\n","            next_obs, reward, done, _ = env.step(action)\n","            \n","            # save into the history\n","            current_traj += [(obs, action, reward)]\n","\n","            obs = next_obs\n","        history += [current_traj]\n","        \n","    return history\n","\n","def calculate_return(rewards: List[float]) -> Tuple[float, List[float]]:\n","    \"\"\"Calulated and episode and step returns\"\"\"\n","    # calculate the sum of rewards from the episode\n","    rewards = np.array(rewards)\n","    episode_return = np.sum(rewards)\n","    \n","    # prepare a list for the step returns\n","    step_returns = []\n","\n","    # calculate discounted return for each step\n","    # hint: it's easier to go backwards\n","    step_returns = [rewards[-1]]\n","    for reward in reversed(rewards[:-1]):\n","        last_return = step_returns[-1]\n","        step_returns += [reward + last_return]\n","    step_returns.reverse()\n","\n","    return episode_return, step_returns\n","\n","def process_trajectories(history: List):\n","    \"\"\"Process gathered trajectories into tensors and calculate returns\"\"\"\n","    # prepare containers for each element\n","    obs_array = []\n","    action_array = []\n","    return_array = []\n","    episode_returns = []\n","    \n","    # loop over the whole history\n","    rewards = []\n","    for traj_idx, traj in enumerate(history):\n","        # unpack the elements\n","        traj_obs, traj_actions, traj_rewards = list(zip(*traj))\n","\n","        # process the end of an episode - calculate episode and step returns\n","\n","        episode_return, step_returns = calculate_return(traj_rewards)\n","        \n","        episode_returns += [episode_return]\n","        obs_array += traj_obs\n","        action_array += traj_actions\n","        return_array += step_returns\n","\n","    # cast out data to tensors (will be useful later)     \n","    obs_array = torch.tensor(obs_array, dtype=torch.float32)\n","    action_array = torch.tensor(action_array, dtype=torch.float32)\n","    return_array = torch.tensor(return_array, dtype=torch.float32)\n","    episode_returns = torch.tensor(episode_returns, dtype=torch.float32)\n","    \n","    return obs_array, action_array, return_array, episode_returns\n","\n","def visualize(env, policy):\n","    \"\"\"Run the provided policy on the environment\"\"\"\n","\n","    env = wrap_env(env)\n","    obs = env.reset()\n","    done = False\n","    \n","    while not done:\n","        action = policy.sample(obs) # ???\n","        obs, reward, done, _ = env.step(action)\n","        env.render()\n","\n","    env.close()\n","    show_video()\n","\n","\n","class NetworkPolicy(nn.Module):\n","\n","    def __init__(self, obs_dim: int, action_dim: int, h_dim: int = 16):\n","        super(NetworkPolicy, self).__init__()\n","\n","        self.model = nn.Sequential(nn.Linear(obs_dim, h_dim),\n","                                   nn.Tanh(),\n","                                   nn.Linear(h_dim, action_dim))\n","\n","    def probs(self, obs):\n","        # cast the numpy array to a torch tensor if necessary\n","        if not isinstance(obs, torch.Tensor):\n","            obs = torch.tensor(obs, dtype=torch.float32)\n","        # get logits from the model\n","        logits = self.model(obs)\n","        # use softmax function to transform logits into probability distribution\n","        return F.softmax(logits, -1)\n","\n","    def log_probs(self, obs: np.ndarray):\n","        # cast the numpy array to a torch tensor if necessary\n","        if not isinstance(obs, torch.Tensor):\n","            obs = torch.tensor(obs, dtype=torch.float32)\n","        # get logits from the model\n","        logits = self.model(obs)\n","        # use *log* softmax function to transform logits into probability distribution\n","        return F.log_softmax(logits, -1)\n","        \n","    def sample(self, obs):\n","        # again, sample from the prepared probability vector \n","        # remember the `.item()` method!\n","        probs = self.probs(obs)\n","        return torch.multinomial(probs, 1).item()\n","\n","\n","def policy_gradient_step(policy: NetworkPolicy,\n","                         optimizer: torch.optim.Optimizer, \n","                         obs: torch.Tensor, \n","                         actions: torch.Tensor, \n","                         step_returns: torch.Tensor,\n","                         num_trajs: int):\n","\n","    # pass the obs to the policy to get log probabilities of each action\n","    log_probs = policy.log_probs(obs)\n","    \n","    # get the probability of the action thast was actual performed for each observation\n","    actions = actions.view(-1, 1).long()\n","    action_log_probs = log_probs.gather(1, actions).squeeze() \n","    # calculat the gradient\n","    target = -(action_log_probs * step_returns).sum() / num_trajs\n","    # pass it to the optimizer\n","    optimizer.zero_grad()\n","    target.backward()\n","    optimizer.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ku6IzEQWcKHt","colab_type":"text"},"source":["# Part 3. Values Function\n","\n","(If you didn't finish the last code part or want to play around with the code more, now is a good chance as this part is significantly smaller than the last one)\n","\n","\n","## Exercise: Average Baseline\n","Change the function `train_policy_gradient` so that it normalizes the `step_return` by subtracting the mean return. Compare the version with normalization and without it. Do you see an improvement?\n"]},{"cell_type":"code","metadata":{"id":"oGtApY_gtsyv","colab_type":"code","colab":{}},"source":["def train_policy_gradient(env: gym.Env, \n","                          policy: torch.nn.Module, \n","                          num_iterations: int = 100, \n","                          trajs_per_gather: int = 10,\n","                          normalize=True):\n","\n","    # we'll use adam to update the weights of our network\n","    optimizer = torch.optim.Adam(policy.parameters(), lr=5e-3)\n","    # training loop\n","    for idx in range(num_iterations + 1):\n","        # gather trajectories using current policy\n","        history = gather_trajectories(env, policy, num_trajs=trajs_per_gather) # ???\n","        \n","        # calculate the obs, actions and returns array by processing the trajectories\n","        obs, actions, step_returns, ep_returns = process_trajectories(history) # ???\n","\n","        # normalize the returns (i.e. substract the mean)\n","        if normalize:\n","            step_returns = step_returns - step_returns.mean() # ???\n","\n","        # policy gradient training\n","        policy_gradient_step(policy=policy,\n","                                optimizer=optimizer,\n","                                obs=obs,\n","                                actions=actions,\n","                                step_returns=step_returns,\n","                                num_trajs=trajs_per_gather)\n","        # log traning progress\n","        if idx % 10 == 0:\n","            print(f\"Traning iteration {idx}, mean episode returns: {ep_returns.mean():.3f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"96876aee-e3b0-4e40-c24f-889c86f70824","executionInfo":{"status":"ok","timestamp":1574428132286,"user_tz":-60,"elapsed":70938,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"id":"4d_ki77-vuqN","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# moon lander\n","# env = gym.make(\"LunarLander-v2\")\n","# or cart pole\n","env = gym.make(\"CartPole-v1\")\n","\n","# gather necessary dimensions for our netowrk\n","obs_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","# initialize the policy\n","network_policy = NetworkPolicy(obs_dim, action_dim)\n","\n","# train the model\n","train_policy_gradient(env, \n","                      network_policy, \n","                      num_iterations=100,\n","                      normalize=True, \n","                      trajs_per_gather=20)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Traning iteration 0, mean episode returns: -186.868\n","Traning iteration 10, mean episode returns: -140.580\n","Traning iteration 20, mean episode returns: -150.856\n","Traning iteration 30, mean episode returns: -119.842\n","Traning iteration 40, mean episode returns: -124.408\n","Traning iteration 50, mean episode returns: -121.022\n","Traning iteration 60, mean episode returns: -129.815\n","Traning iteration 70, mean episode returns: -102.378\n","Traning iteration 80, mean episode returns: -110.217\n","Traning iteration 90, mean episode returns: -125.085\n","Traning iteration 100, mean episode returns: -91.546\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WTyaCc1qt7l3","colab_type":"text"},"source":["## Exercise: Building a Simple Network.\n","\n","Here we will implement Policy Gradient training with a baseline function. We'll use a separate *Value Network* to estimate values $V_{\\pi_\\theta}$, implement its training procedure and use it in the Policy Gradient algorithm.\n","\n","Your task is to create a simple fully connected neural network using pytorch's [`nn.Sequential`](https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential) interface with three [`nn.Linear`](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) layers and [`nn.Tanh`](https://pytorch.org/docs/stable/nn.html#torch.nn.Tanh) activations functions. In other words, the network should have 2 hidden layers, both os size `h_dim`, should be albe to process environment observations and return a single scalar.\n","\n"]},{"cell_type":"code","metadata":{"id":"-Ie269Qqm0OL","colab_type":"code","colab":{}},"source":["def get_value_network(env: gym.Env, h_dim: int = 32):\n","    \"\"\"Create a value network with 2 hidden layers, both with `h_dim` neurons\n","       and Tanh nonlinear activations\"\"\"\n","\n","    obs_dim = env.observation_space.shape[0]\n","    \n","    # build the network\n","    value_network = nn.Sequential( # ???\n","        nn.Linear(obs_dim, h_dim),\n","        nn.Tanh(),\n","        nn.Linear(h_dim, h_dim),\n","        nn.Tanh(),\n","        nn.Linear(h_dim, 1))\n","    \n","    return value_network"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKWRcoh9vXGX","colab_type":"text"},"source":["## Exercise: Value Network training step\n","\n","Now we need a way to train our Value Network. As we know its task is to predict a return $R_t$ for a given state $s_t$. We already have gathered those for our policy gradient so let's feed it into out Value network.\n","\n","As a reminder, we will use the Mean Squared Error as the loss function to train the Value Network:\n","\n","$$ \\frac{1}{M}\\sum_{j=1}^{N}\\sum_{t=1}^T\\big(R_t(\\tau_j)-V_\\psi(s_t(\\tau_j))\\big)^2 $$\n","        where $M$ is the total number of samples in all trajectories.\n","\n","The function `value_net_step` receives pair of vectors containing $s_i$ and $R_i$, so all you need to do here is to calculate and minimize:\n","\n","$$ \\frac{1}{M}\\sum_{i=1}^{M}\\big(R_i -V_\\psi(s_i)\\big)^2 $$\n"]},{"cell_type":"code","metadata":{"id":"tkcEKVdnwIBW","colab_type":"code","colab":{}},"source":["def value_net_step(obs: torch.Tensor, \n","                   step_returns: torch.Tensor,\n","                   model: torch.nn.Module, \n","                   optim: torch.optim.Optimizer):\n","    \"\"\"\"Train the value network on a single batch of states and returns\"\"\"\n","    \n","    # pass the observatrion to get network and get the predicted values\n","    values = model(obs).squeeze() # ???\n","\n","    # calculate the loss function: mean squared error\n","    loss = ((values - step_returns) ** 2).mean() # ???\n","    \n","    # pass gradients to the optimizer\n","    optim.zero_grad()\n","    loss.backward()\n","    optim.step()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9KbzTcM-yup4","colab_type":"text"},"source":["Next we can reuse our `policy_gradient_step` function from previous part, all we need to do is to modify the `step_returns` passed. \n","\n","As a reminder we want to replace the returns $R_t$ with advantages  $A_t = R_t - V_t$ where $V_t$ is the value predicted by our new network.\n"]},{"cell_type":"code","metadata":{"id":"k1xpkd0yv7BE","colab_type":"code","colab":{}},"source":["def train_pg_baseline(env: gym.Env, \n","                      policy: torch.nn.Module, \n","                      value_network: torch.nn.Module, \n","                      num_iterations: int = 100, \n","                      value_net_epochs: int = 20,\n","                      trajs_per_gather: int = 10):\n","    \n","    # prepare optimizers for both networks\n","    policy_optimizer = torch.optim.Adam(policy.parameters(), lr=5e-3)\n","    value_optimizer = torch.optim.Adam(value_network.parameters(), lr=5e-1)\n","    \n","    # training loop\n","    for idx in range(num_iterations + 1):\n","        \n","        # gather trajectories using current policy\n","        history = gather_trajectories(env, policy, num_trajs=trajs_per_gather)\n","        # calculate the obs, actions and returns array by processing the trajectories\n","        obs, actions, step_returns, ep_returns = process_trajectories(history)\n","        \n","        # now modify step_returns using the baseline from value network\n","        # here you need to use `.detach()` method to detach the value\n","        # network's output from the gradient calculation graph\n","        advantage = step_returns - value_network(obs).detach().squeeze() # ???\n","\n","        # first train the value network\n","        for val_idx in range(value_net_epochs):\n","            value_net_step(obs=obs,  # ???\n","                        step_returns=step_returns, \n","                        model=value_network, \n","                        optim=value_optimizer) \n","        \n","\n","        # run the policy gradient step like before\n","        policy_gradient_step(policy=policy,\n","                         optimizer=policy_optimizer, \n","                         obs=obs, \n","                         actions=actions, \n","                         step_returns=advantage,\n","                         num_trajs=trajs_per_gather)\n","    \n","        # log training progress\n","        if idx % 10 == 0:\n","            print(f\"Traning iteration {idx}, mean episode returns: {ep_returns.mean():.3f}\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6hjisDk4w00","colab_type":"text"},"source":["All that's left is to run our Policy Gradient with a baseline functions."]},{"cell_type":"code","metadata":{"id":"2CFl8tYs4xBV","colab_type":"code","outputId":"92f13d3b-aeeb-473c-caea-f0d4e1d1a44d","executionInfo":{"status":"ok","timestamp":1574428242871,"user_tz":-60,"elapsed":74323,"user":{"displayName":"Igor Sieradzki","photoUrl":"","userId":"10906567088243523558"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# moon lander\n","# env = gym.make(\"LunarLander-v2\")\n","# or cart pole\n","env = gym.make(\"CartPole-v1\")\n","\n","# gather necessary dimensions for our netowrk\n","obs_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","\n","# initialize the policy\n","network_policy = NetworkPolicy(obs_dim, action_dim)\n","value_network = get_value_network(env)\n","\n","# train the model\n","train_pg_baseline(env, \n","    policy=network_policy, \n","    value_network=value_network,\n","    num_iterations=100,\n","    trajs_per_gather=20\n",")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Traning iteration 0, mean episode returns: -192.894\n","Traning iteration 10, mean episode returns: -147.789\n","Traning iteration 20, mean episode returns: -150.770\n","Traning iteration 30, mean episode returns: -149.115\n","Traning iteration 40, mean episode returns: -125.745\n","Traning iteration 50, mean episode returns: -110.416\n","Traning iteration 60, mean episode returns: -121.051\n","Traning iteration 70, mean episode returns: -109.723\n","Traning iteration 80, mean episode returns: -113.034\n","Traning iteration 90, mean episode returns: -115.072\n","Traning iteration 100, mean episode returns: -112.935\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CGvDXbIS3i_P","colab_type":"text"},"source":["## Bonus Exercise 1\n","Try to play with `value_net_epochs` to find balance between computation cost and sample efficiency. How many value network epochs are needed for it to perform better than the simple average baseline?\n","\n","## Bonus Exercise 2\n","It might be a good idea for value network and policy to share some parameters, since they both need to extract important features from the data.\n","\n","Build a neural network such that:\n","1. The first two layers process the state $s$ to get the representation $f(s)$\n","2. The representation $f(s)$ is then passed to the policy network which, based on that, returns the vector or probabilities $\\pi_\\theta$.\n","3. The representation $f(s)$ is also passed to the value network which returns the value $V_\\psi(s)$.\n","\n","Try to train the whole network end-to-end, so that the first two layers are updated both when minimizing the value network loss and policy loss. Do you see an improvement?"]}]}